---
import "../styles/global.css";
import { actions } from "astro:actions";
---

<html lang="en">
  <head>
    <script
      src="https://unpkg.com/amazon-kinesis-video-streams-webrtc/dist/kvs-webrtc.min.js"
    ></script>
    <meta charset="utf-8" />
    <link rel="icon" type="image/svg+xml" href="/favicon.svg" />
    <meta name="viewport" content="width=device-width" />
    <meta name="generator" content={Astro.generator} />
    <title>Astro</title>

    <style>
      #meter {
        width: 300px;
        height: 50px;
        background: #eee;
        border-radius: 4px;
        margin: 20px;
        box-shadow: 0 2px 8px #ccc;
      }
      #level {
        height: 100%;
        background: linear-gradient(to right, #4caf50, #ffeb3b, #f44336);
        border-radius: 4px 0 0 4px;
        transition: width 0.06s;
      }
    </style>
  </head>
  <body>
    <main>
      <div class="m-10 p-10 flex flex-col justify-center align-center">
        <h1>Astro</h1>
        <button
          id="camera"
          class="border-1 border-blue-500 p-2 hover:border-blue-900 w-[250px] cursor-pointer"
          >Open Camera</button
        >
        <video id="video" class="w-[600px] h-[300px]"></video>
        <div class="flex">
          <button
            id="mute"
            class="border-1 border-blue-500 p-2 hover:border-blue-900 w-[250px] cursor-pointer"
            >Mute Video</button
          >
          <div id="meter">
            <div id="level" style="width:0%"></div>
          </div>
        </div>
      </div>
    </main>
    <script>
      import { actions } from "astro:actions";

      const videoEl: HTMLVideoElement = document.getElementById(
        "video"
      )! as HTMLVideoElement;
      const levelDiv = document.getElementById("level")!;
      const muteEl = document.getElementById("mute")!;

      muteEl.addEventListener("click", () => (videoEl.muted = !videoEl.muted));

      let disconnectVisualizer: (() => void) | null = null;
      let signalingClient: any = null;

      function visualizeAudioLevel(mediaStream: MediaStream) {
        const AudioContext = window.AudioContext;
        const audioCtx = new AudioContext();
        const source = audioCtx.createMediaStreamSource(mediaStream);
        const analyser = audioCtx.createAnalyser();
        analyser.fftSize = 256;
        const bufferLength = analyser.frequencyBinCount;
        const dataArray = new Uint8Array(bufferLength);

        source.connect(analyser);

        disconnectVisualizer = () => source.disconnect(analyser);

        function draw() {
          analyser.getByteTimeDomainData(dataArray);

          // Calculate peak (max amplitude deviation from center)
          let max = 0;
          for (let i = 0; i < bufferLength; i++) {
            let v = Math.abs(dataArray[i] - 128);
            if (v > max) max = v;
          }

          // Normalize: max possible deviation is 128
          let percent = Math.min(100, (max / 128) * 100);

          levelDiv.style.width = percent + "%";

          requestAnimationFrame(draw);
        }
        draw();
      }

      document.getElementById("camera")?.addEventListener("click", async () => {
        if (videoEl.srcObject) {
          if (!!disconnectVisualizer) {
            disconnectVisualizer();
          }
          if (signalingClient !== null) {
            signalingClient.close();
          }
          videoEl.srcObject = null;
          return;
        }

        try {
          const stream = await navigator.mediaDevices.getUserMedia({
            video: true,
            audio: true,
          });

          visualizeAudioLevel(stream);
          videoEl.srcObject = stream;
          videoEl.play();

          const { data, error } = await actions.getChannelEndpoints({});

          console.log(data);

          if (!data || error) {
            throw new Error("failed to get channel endpoints");
          }

          signalingClient = new (window as any).KVSWebRTC.SignalingClient({
            channelARN: import.meta.env.PUBLIC_AWS_CHANNEL_ARN,
            channelEndpoint: data.WSS,
            role: "MASTER",
            region: import.meta.env.PUBLIC_AWS_REGION,
            credentials: {
              accessKeyId: import.meta.env.PUBLIC_AWS_ACCESS_KEY,
              secretAccessKey: import.meta.env.PUBLIC_AWS_SECRET_ACCESS_KEY,
            },
          });

          let peerConnection: RTCPeerConnection;

          const configuration = {
            iceServers: [
              { urls: "stun:stun.kinesisvideo.us-east-1.amazonaws.com:443" },
              // add TURN servers if needed
            ],
          };
          peerConnection = new RTCPeerConnection(configuration);

          stream.getTracks().forEach((track) => {
            peerConnection.addTrack(track, stream);
          });

          signalingClient.on("open", async () => {
            const offer = await peerConnection.createOffer({
              offerToReceiveAudio: true,
              offerToReceiveVideo: true,
            });
            await peerConnection.setLocalDescription(offer);
            signalingClient.sendSdpOffer(peerConnection.localDescription);
          });

          // When you receive the SDP answer
          signalingClient.on(
            "sdpAnswer",
            async (answer: RTCSessionDescriptionInit) => {
              await peerConnection.setRemoteDescription(answer);
              remoteDescriptionSet = true;
              // Add all buffered ICE candidates
              for (const candidate of remoteIceCandidates) {
                await peerConnection.addIceCandidate(candidate);
              }
              remoteIceCandidates.length = 0; // Clear the buffer
            }
          );

          // Buffer for ICE candidates received before remote description is set
          const remoteIceCandidates: RTCIceCandidateInit[] = [];
          let remoteDescriptionSet = false;

          // When you receive an ICE candidate from signaling
          signalingClient.on(
            "iceCandidate",
            (candidate: RTCIceCandidateInit) => {
              if (remoteDescriptionSet) {
                peerConnection.addIceCandidate(candidate);
              } else {
                remoteIceCandidates.push(candidate);
              }
            }
          );

          peerConnection.onicecandidate = ({ candidate }) => {
            if (candidate) {
              signalingClient.sendIceCandidate(candidate);
            }
          };

          signalingClient.open();
        } catch (err) {
          console.log(err);
          /* handle the error */
        }
      });
    </script>
  </body>
</html>
